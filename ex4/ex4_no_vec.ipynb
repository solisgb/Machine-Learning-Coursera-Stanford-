{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 Neural Network Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_layer_size = 400, hidden_layer_size = 25, num_labels = 10\n"
     ]
    }
   ],
   "source": [
    "input_layer_size  = 400;  % 20x20 Input Images of Digits\n",
    "hidden_layer_size = 25;   % 25 hidden units\n",
    "num_labels = 10;          % 10 labels, from 1 to 10 (we have mapped \"0\" to label 10)\n",
    "fprintf('input_layer_size = %d, hidden_layer_size = %d, num_labels = %d\\n',input_layer_size, hidden_layer_size, num_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Loading and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = size(X, 1) = 5000 "
     ]
    }
   ],
   "source": [
    "load('ex4data1.mat');\n",
    "m = size(X, 1);\n",
    "fprintf('m = size(X, 1) = %d %d\\n', size(X, 1));\n",
    "\n",
    "% Randomly select 100 data points to display\n",
    "%sel = randperm(size(X, 1));\n",
    "%sel = sel(1:100);\n",
    "\n",
    "graphics_toolkit ('gnuplot');\n",
    "%displayData(X(sel, :));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Loading Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Load the weights into variables Theta1 and Theta2\n",
    "load('ex4weights.mat');\n",
    "\n",
    "% Unroll parameters \n",
    "nn_params = [Theta1(:) ; Theta2(:)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Compute Cost (Feedforward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m = size(X, 1) = 5000\n"
     ]
    }
   ],
   "source": [
    "fprintf('m = size(X, 1) = %d\\n', size(X, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "% Weight regularization parameter (we set this to 0 here).\n",
    "lambda = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size Theta1 = 25 401; size Theta2 = 10 26\n"
     ]
    }
   ],
   "source": [
    "fprintf('size Theta1 = %d %d; size Theta2 = %d %d\\n', size(Theta1), size(Theta2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brack propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Theta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ...\n",
    "                 hidden_layer_size, (input_layer_size + 1));\n",
    "Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ...\n",
    "                 num_labels, (hidden_layer_size + 1));\n",
    "\n",
    "m = size(X, 1);\n",
    "         \n",
    "J = 0;\n",
    "Theta1_grad = zeros(size(Theta1)); % 25 * 401\n",
    "Theta2_grad = zeros(size(Theta2)); % 10 * 26\n",
    "\n",
    "X_add1 = [ones(m,1) X];\n",
    "\n",
    "%%% old\n",
    "activation2 = sigmoid(X_add1 * Theta1');  %  Old: 5000 * 401 * 401 * 25\n",
    "activation2 = [ones(m,1) activation2]; %  5000 * 26\n",
    "Htheta = sigmoid(activation2 * Theta2'); % 5000 * 10, suit Htheta(i,k) \n",
    "\n",
    "eyebase = eye(num_labels);\n",
    "y_matrix = eyebase(y,:); \n",
    "\n",
    "\n",
    "for i = 1 : m\n",
    "    for k = 1 : num_labels\n",
    "        J = J + ( - y_matrix(i,k) * log(Htheta(i,k)) - (1 - y_matrix(i,k)) * log(1 - Htheta(i,k)));\n",
    "    end\n",
    "end\n",
    "\n",
    "J = 1/m * J;  \n",
    "\n",
    "% -------------------------------------------------------------\n",
    "\n",
    "%%% ex4 1.4 Regularized Cost Function\n",
    "\n",
    "regTerm_Theta1 = 0;\n",
    "regTerm_Theta2 = 0;\n",
    "\n",
    "for j = 1 : hidden_layer_size\n",
    "    for k = 1 : input_layer_size\n",
    "        regTerm_Theta1 = regTerm_Theta1 + Theta1(j,k+1)^2;\n",
    "    end\n",
    "end\n",
    "\n",
    "for j = 1 : num_labels\n",
    "    for k = 1 : hidden_layer_size\n",
    "        regTerm_Theta2 = regTerm_Theta2 + Theta2(j,k+1)^2;\n",
    "    end\n",
    "end\n",
    "\n",
    "J = J + lambda/(2*m) * (regTerm_Theta1 + regTerm_Theta2);\n",
    "\n",
    "% Part 2: Implement the backpropagation algorithm to compute the gradients\n",
    "%         Theta1_grad and Theta2_grad. You should return the partial derivatives of\n",
    "%         the cost function with respect to Theta1 and Theta2 in Theta1_grad and\n",
    "%         Theta2_grad, respectively. After implementing Part 2, you can check\n",
    "%         that your implementation is correct by running checkNNGradients\n",
    "%\n",
    "%         Note: The vector y passed into the function is a vector of labels\n",
    "%               containing values from 1..K. You need to map this vector into a \n",
    "%               binary vector of 1's and 0's to be used with the neural network\n",
    "%               cost function.\n",
    "%\n",
    "%         Hint: We recommend implementing backpropagation using a for-loop\n",
    "%               over the training examples if you are implementing it for the \n",
    "%               first time.\n",
    "\n",
    "\n",
    "delta_3 = zeros(num_labels, 1); % 10 * 1\n",
    "\n",
    "BigDelta_2 = zeros(size(Theta2_grad));\n",
    "BigDelta_1 = zeros(size(Theta1_grad));\n",
    "\n",
    "% for loop for step 1 to 4\n",
    "for t = 1 : m\n",
    "    for k = 1 : num_labels\n",
    "\n",
    "        %%% Lab step 1:\n",
    "        % denote a^{(l)} (the lth layer) as a_l. (al normally used as elemental 'fen liang') \n",
    "\n",
    "        a_1 = X(t,:)'; %  column vector\n",
    "        a_1 = [1;a_1]; % 401 * 1\n",
    "        z_2 = Theta1 * a_1; % 25 * 401 * 401 * 1 = 25 * 1\n",
    "        a_2 = sigmoid(z_2); \n",
    "        a_2 = [1;a_2]; % 26 * 1\n",
    "        z_3 = Theta2 * a_2; % 10 * 26 * 26 * 1 = 10 * 1\n",
    "        a_3 = sigmoid(z_3); % 10 * 1\n",
    "\n",
    "\n",
    "        %%% Lab step 2:               \n",
    "        delta_3(k) = a_3(k) - y_matrix(t,k); \n",
    "    end\n",
    "    \n",
    "        %%% Lab step 3:\n",
    "        % delta_2 = zeros(hidden_layer_size,1);        \n",
    "        %% 1st try, a little different than lab note p.9 in terms of notation (ignore bias unit in Theta instead of delta)      \n",
    "        delta_2 = Theta2(:,2:end)' * delta_3 .* sigmoidGradient(z_2);  % 25 * 10 * 10 * 1 .* 25 * 1\n",
    "        % ignoring the Theta2 bias units\n",
    "\n",
    "  \n",
    "        %%% Lab step 4:\n",
    "\n",
    "        BigDelta_2 = BigDelta_2 + delta_3 * a_2';  % 10 * 1 * 1 * 26 = 10 * 26  num_labels * hidden_layer_size\n",
    "        BigDelta_1 = BigDelta_1 + delta_2 * a_1';  % 25 * 1 * 1 * 401 = 25 * 401  hidden_layer_size * input_layer_size\n",
    "    \n",
    "end\n",
    "\n",
    "fprintf('size(BigDelta_2) %d %d, size(BigDelta_2) %d %d',size(BigDelta_2), size(BigDelta_1))\n",
    "% -------------------------------------------------------------\n",
    "\n",
    "\n",
    "%%% Lab step 5:\n",
    "\n",
    "Theta1_grad = BigDelta_1 / m;\n",
    "\n",
    "Theta2_grad = BigDelta_2 / m;\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Octave",
   "language": "octave",
   "name": "octave"
  },
  "language_info": {
   "file_extension": ".m",
   "help_links": [
    {
     "text": "GNU Octave",
     "url": "https://www.gnu.org/software/octave/support.html"
    },
    {
     "text": "Octave Kernel",
     "url": "https://github.com/Calysto/octave_kernel"
    },
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-octave",
   "name": "octave",
   "version": "6.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
